model:
  activation: "relu"
  initialization: "kaiming"
  n_heads: 2
  prenormalization: false
  attention_dropout: 0.2
  d_token: 32
  n_layers: 3
  residual_dropout: 0.2
training:
  lr: 5.0e-4    
  weight_decay: 2.0e-5
  optimizer: "adamw"
